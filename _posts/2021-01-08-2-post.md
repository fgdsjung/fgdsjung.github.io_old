---
title: "k-NN 알고리즘"
date: 2021-01-08 16:21:00 -0400
categories: ml 머신러닝
---
k-NN(Nearest Neighbors) 알고리즘은 가장 간단한 머신러닝 알고리즘이며 단순히 훈련 데이터셋을 저장해 모델을 만든다. 
새로운 데이터에 대해 예측할때에는 훈련 데이터셋에서 가장 가까운 데이터 포인트인 최근접 이웃을 찾는다.

이웃이 1개일 경우 최근접 이웃에 해당하는 클래스로 예측한다.
이웃이 여러 개 일 경우 분류는 최근접 이웃 중 투표를 통해 이웃이 많은 클래스로 예측하고 회귀는 최근접 이웃 간의 단순 평균이나 거리를 고려한 가중치 평균으로 값을 예측한다.

일반적으로 k-NN 분류기에 중요한 하이퍼파라미터는 데이터 포인트 사이의 거리를 재는 방법과 이웃 수이며 이웃 수는 홀수로 3이나 5일때 잘 작동하지만 설정을 잘 해야 한다.
거리를 재는 방법은 기본적으로 여러 환경에서 잘 동작하는 유클리디안 거리 방식을 사용한다.

장점
- 이해하기 쉽고 설정을 많이 바꾸지 않아도 좋은 성능을 내며 더 복잡한 알고리즘을 적용하기전에 시도하기 좋은 접근방법이다.

단점
- 모델을 빠르게 만들 수 있지만 훈련 데이터셋이 매우 크면(특성 수나 샘플 수) 예측이 느려지며 데이터 전처리가 매우 중요하다.(이웃 간의 거리를 계산할 때 특성마다 값의 범위가 다르면
범위가 작은 특성에 크게 영향을 받는다).
- 수백 개 이상의 특성을 가진 데이터셋에는 잘 동작하지 않으며 특성 값 대부분이 0인 데이터셋에서도 잘 동작하지 않는다.

즉 k-NN 알고리즘은 이해하긴 쉽지만 예측이 느리고 많은 특성을 처리하는 능력이 부족해 잘 쓰지는 않는다.
