---
title: "모델파라미터, 하이퍼파라미터, 선형 모델"
date: 2021-01-08 17:21:00 -0400
categories: ml 머신러닝
---
모델 파라미터란?
- 머신러닝에서 알고리즘이 주어진 데이터로부터 학습하는 파라미터를 말한다. 파라미터 혹은 계수라고 부르기도 한다.

하이퍼파라미터란?
- 모델이 학습할 수 없어서 직접 설정해 주어야 하는 파라미터를 말한다.(매개 변수)

### 선형 모델
입력 특성에 대한 선형 함수를 만들어 예측을 수행한다.

#### 회귀의 선형 모델
회귀는 연속적인 숫자나 실수(부동소수점수)를 예측하는 것을 말한다. 예상하는 출력 값들 사이에 연속성이 있다면 회귀 문제이다. 예를 들어 연간 소득을 예측하거나(작년, 올해, 내년) 
농작물의 올해 수확량을 예측하는 것은 회귀 문제이다.

선형 회귀 모델을 위한 일반화된 예측 함수는 다음과 같다.

`ŷ = w[0] x X[0] + w[1] x X[1] + ... + w[p] x X[p] + b`

X는 하나의 샘플에 대한 p + 1개의 특성들을 나타내며 w와 b는 모델이 학습할 파라미터(계수)이고 ŷ는 모델이 만들어낸 예측 값이다.
위의 식에서 w는 기울기, b는 y축과 만나는 절편이며 w는 각 특성 X에 해당하는 기울기를 각각 가지고 있다. 따라서 예측 값은 입력 특성 X에 w의 각 가중치를 곱해서 더한 가중치 합으로 볼 수 있다.

선형 회귀 모델은 특성이 하나 일 경우 직선, 두 개일 경우 평면, 그 이상일 경우 초평면(hyperplane)이 되는 특징을 가지고 있다.

#### 선형 회귀(최소제곱법)
선형 회귀(linear regression)는 예측과 훈련 데이터셋의 타깃 y 사이의 평균제곱오차를 최소화하는 파라미터 w와 b를 찾는다. 
**평균제곱오차** 는 예측값과 타깃값의 차이를 제곱하여 더한 후 샘플의 개수로 나눈 것이다. 

선형 회귀는 매개변수가 없어 따로 설정해야 하는 값이 없는게 장점이지만 모델의 복잡도를 제어할 방법도 없다.

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

X_train, y_train, X_test, y_test = tarin_test_split(X, y, random_state=42)

lr = LinearRegression().fit(X_tarin, y_tarin)
```
기울기 파라미터(w)는 가중치(weight) 또는 계수(coefficient)라고 하며 lr.coef_ 에 저장되어 있고 편향(offset) 또는 절편(intercept) 파라미터(b)는 lr.intercept_ 에 저장되어 있다.

특성이 많은 고차원 데이터셋에서는 선형 모델의 성능이 매우 높아져 과대적합될 가능성이 높다. 이런 경우 모델의 복잡도를 제어할 수 있는 모델을 사용해야 하며 기본 선형 회귀 대신
가장 널리 쓰이는 모델이 릿지(Ridge) 회귀이다.

#### 릿지 회귀
선형 모델이므로 최소제곱법을 사용한 것과 같은 예측 함수를 사용한다. 가중치(w)를 설정할 수 있으며 가중치의 절댓값을 가능한 작게 만들어 w의 모든 원소가 0에 가깝게 만든다.
즉 이를 통해 모든 특성이 출력에 주는 영향을 최소한으로 만들 수 있다.(기울기를 작게 만든다)

이런 제약을 **규제(regularization)** 라고 하며 모델이 과대적합 되지 않도록 강제로 제한한다는 의미이다.

릿지 회귀에서 사용하는 규제 방식을 **L2 규제**라고 하며 매개변수 𝛼 값을 크게하면 페널티의 효과가 커져 가중치가 감소하고 𝛼 값을 작게하면 페널티 효과가 작아져 가중치가 커진다.

- alpha(𝛼) 값 증가 --> 페널티 증가 --> 가중치 감소 --> 계수를 0에 더 가깝게 만듬 --> 기울기 작아짐 --> 모델 복잡도 낮아짐 --> 훈련 점수 낮아지고 시험 점수 높아짐 --> 과대적합이 적어짐 --> 더 일반화된 모델이 됨. 

```
from sklearn.linear_model import Ridge

# alpha 기본값: 1.0
ridge = Ridge(alpha=10).fit(X_train, y_train)
ridge.score(X_train, y_train)
ridge.score(X_test, y_test)
```
- 높은 alpha 값 = 작은 ridge.coef_ 절대 값

규제의 효과를 이해하기 위해서 alpha 값을 고정하고 훈련 데이터셋의 크기를 변화시켜 성능을 확인(학습 곡선)해 볼 수 있다.
- **학습 곡선(learning curve)**: 데이터셋의 크기에 따른 모델의 성능 변화를 나타낸 그래프이다. 훈련 과정을 여러 번 반복하면서 학습하는 알고리즘에서는 반복의 횟수에 따른 성능 변화를 나타내는 그래프를 말한다.

보스턴 주택가격 데이터셋의 크기를 변화시켜 alpha 값을 고정한 릿지와 선형 회귀를 적용했을 경우
1. 둘다 훈련 점수가 테스트 점수보다 높다.
2. 릿지는 규제가 적용되므로 훈련 점수가 전체적으로 선형 회귀의 훈련 점수보다 낮다.
3. 시험 점수는 릿지가 선형 회귀보다 더 높으며 특별히 작은 데이터셋에서는 더 높다.
4. 데이터가 많아질수록 두 모델 모두 성능이 좋아지고 마지막에는 선형 회귀가 릿지 회귀의 성능을 따라잡는다.

즉 데이터를 충분히 주면 규제 항은 덜 중요해져서 릿지 회귀와 선형 회귀의 시험 점수 성능은 같아진다. 또한 데이터가 많아질수록 모델이 데이터를 기억하거나 과대적합하기 어려워지기 때문에 선형 회귀의 훈련 점수는 낮아진다.

#### 라쏘 회귀
Lasso 도 선형 회귀에 규제를 적용하는 모델이며 릿지 회귀와 같이 계수를 0에 가깝게 만들며 어떤 계수는 정말 0이 되게 한다.

라쏘는 **L1 규제** 방식을 사용하며 L1 규제의 결과로 어떤 계수는 0이 되어(릿지는 어떤 계수도 0이 되지 않는다) 모델에서 완전히 제외되는 특성이 생긴다.(특성 선택)
라쏘는 전체에서 일부 특성들만 선별해서 사용하는 것과 유사하며 좀 더 적은 특성을 사용하면 과대적합 개선에 도움이 된다.

- alpha(𝛼) 값 증가 --> 페널티 증가 --> 가중치 감소 --> 일부 계수는 0이 됨 --> 일부 특성이 제외됨 --> 모델 복잡도 낮아짐 --> 과대적합이 적어짐 --> 더 일반화된 모델이 됨.

```
from sklearn.linear_model import Lasso

# alpha 기본값: 1.0
lasso = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)
# 사용된 특성 수 확인
np.sum(lasso.coef_ != 0)
```

라쏘에서 과소적합을 줄이기 위해 alpha 값을 줄일 수 있으며 alpha 값을 줄이면 max_iter의 기본 값을 늘려야 한다.

릿지와 라쏘 중 일반적으로 릿지를 더 선호하지만 특성이 많고 그 중 일부분만 중요하거나 분석하기 쉬운 모델을 원한다면 라쏘를 사용한다. 
